{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of distribution reweighting\n",
    "\n",
    "**hep_ml.reweight** contains methods to reweight distributions. \n",
    "Typically we use reweighting of monte-carlo to fight drawbacks of simulation, though there are many applications.\n",
    "\n",
    "In this example we reweight multidimensional distibutions: `original` and `target`, the aim is to find new weights for original distribution, such that these multidimensional distributions will coincide. \n",
    "\n",
    "Here we have a __toy example__ without a real physics meaning.\n",
    "\n",
    "Pay attention: equality of distibutions for each feature $\\neq$ equality of multivariate distributions.\n",
    "\n",
    "All samples are divided into **training** and **validation** part. Training part is used to fit reweighting rule and test part is used to estimate reweighting quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import root_numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from hep_ml import reweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = \"https://github.com/arogozhnikov/hep_ml/blob/data/data_to_download/\"\n",
    "!wget -O ../data/MC_distribution.root -nc $storage/MC_distribution.root?raw=true\n",
    "!wget -O ../data/RD_distribution.root -nc $storage/RD_distribution.root?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"hSPD\", \"pt_b\", \"pt_phi\", \"vchi2_b\", \"mu_pt_sum\"]\n",
    "\n",
    "original = root_numpy.root2array(\"../data/MC_distribution.root\", branches=columns)\n",
    "target = root_numpy.root2array(\"../data/RD_distribution.root\", branches=columns)\n",
    "\n",
    "original = pandas.DataFrame(original)\n",
    "target = pandas.DataFrame(target)\n",
    "\n",
    "original_weights = numpy.ones(len(original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare train and test samples\n",
    "\n",
    "* train part is used to train reweighting rule\n",
    "* test part is used to evaluate reweighting rule comparing the following things: \n",
    "    * Kolmogorov-Smirnov distances for 1d projections\n",
    "    * n-dim distibutions using ML (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide original samples into training ant test parts\n",
    "original_train, original_test = train_test_split(original)\n",
    "# divide target samples into training ant test parts\n",
    "target_train, target_test = train_test_split(target)\n",
    "\n",
    "original_weights_train = numpy.ones(len(original_train))\n",
    "original_weights_test = numpy.ones(len(original_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hep_ml.metrics_utils import ks_2samp_weighted\n",
    "\n",
    "hist_settings = {\"bins\": 100, \"density\": True, \"alpha\": 0.7}\n",
    "\n",
    "\n",
    "def draw_distributions(original, target, new_original_weights):\n",
    "    plt.figure(figsize=[15, 7])\n",
    "    for id, column in enumerate(columns, 1):\n",
    "        xlim = numpy.percentile(numpy.hstack([target[column]]), [0.01, 99.99])\n",
    "        plt.subplot(2, 3, id)\n",
    "        plt.hist(original[column], weights=new_original_weights, range=xlim, **hist_settings)\n",
    "        plt.hist(target[column], range=xlim, **hist_settings)\n",
    "        plt.title(column)\n",
    "        print(\n",
    "            \"KS over \",\n",
    "            column,\n",
    "            \" = \",\n",
    "            ks_2samp_weighted(\n",
    "                original[column],\n",
    "                target[column],\n",
    "                weights1=new_original_weights,\n",
    "                weights2=numpy.ones(len(target), dtype=float),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice:\n",
    "Setting `density=True` in `hist_settings` above means that the histograms will be drawn normalized below, so that the area under each histogram integrates to 1. This can obscure the fact that the weights produced by `predict_weights` are not normalized; if you want the number of effective events to be the same after reweighting, you must renormalize the weights manually--see the `reweight` documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original distributions\n",
    "KS = Kolmogorov-Smirnov distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pay attention, actually we have very few data\n",
    "len(original), len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distributions(original, target, original_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train part of original distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distributions(original_train, target_train, original_weights_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test part for target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_distributions(original_test, target_test, original_weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bins-based reweighting in n dimensions\n",
    "Typical way to reweight distributions is based on bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_reweighter = reweight.BinsReweighter(n_bins=20, n_neighs=1.0)\n",
    "bins_reweighter.fit(original_train, target_train)\n",
    "\n",
    "bins_weights_test = bins_reweighter.predict_weights(original_test)\n",
    "# validate reweighting rule on the test part comparing 1d projections\n",
    "draw_distributions(original_test, target_test, bins_weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Reweighter\n",
    "\n",
    "This algorithm is inspired by gradient boosting and is able to fight curse of dimensionality.\n",
    "It uses decision trees and special loss functiion (**ReweightLossFunction**).\n",
    "\n",
    "**GBReweighter** supports negative weights (to reweight MC to splotted real data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweighter = reweight.GBReweighter(\n",
    "    n_estimators=50, learning_rate=0.1, max_depth=3, min_samples_leaf=1000, gb_args={\"subsample\": 0.4}\n",
    ")\n",
    "reweighter.fit(original_train, target_train)\n",
    "\n",
    "gb_weights_test = reweighter.predict_weights(original_test)\n",
    "# validate reweighting rule on the test part comparing 1d projections\n",
    "draw_distributions(original_test, target_test, gb_weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing some simple expressions:\n",
    "the most interesting is checking some other variables in multidimensional distributions (those are expressed via original variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ks_of_expression(expression):\n",
    "    col_original = original_test.eval(expression, engine=\"python\")\n",
    "    col_target = target_test.eval(expression, engine=\"python\")\n",
    "    w_target = numpy.ones(len(col_target), dtype=\"float\")\n",
    "    print(\n",
    "        \"No reweight   KS:\",\n",
    "        ks_2samp_weighted(col_original, col_target, weights1=original_weights_test, weights2=w_target),\n",
    "    )\n",
    "    print(\n",
    "        \"Bins reweight KS:\", ks_2samp_weighted(col_original, col_target, weights1=bins_weights_test, weights2=w_target)\n",
    "    )\n",
    "    print(\"GB Reweight   KS:\", ks_2samp_weighted(col_original, col_target, weights1=gb_weights_test, weights2=w_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ks_of_expression(\"hSPD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ks_of_expression(\"hSPD * pt_phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ks_of_expression(\"hSPD * pt_phi * vchi2_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ks_of_expression(\"pt_b * pt_phi / hSPD \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ks_of_expression(\"hSPD * pt_b * vchi2_b / pt_phi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB-discrimination\n",
    "let's check how well a classifier is able to distinguish these distributions. ROC AUC is taken as a measure of quality.\n",
    "\n",
    "For this puprose we split the data into train and test, then we train a classifier do distinguish these distributions.\n",
    "If ROC AUC = 0.5 on test, distibutions are identical, if ROC AUC = 1.0, they are ideally separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = numpy.concatenate([original_test, target_test])\n",
    "labels = numpy.array([0] * len(original_test) + [1] * len(target_test))\n",
    "\n",
    "weights = {}\n",
    "weights[\"original\"] = original_weights_test\n",
    "weights[\"bins\"] = bins_weights_test\n",
    "weights[\"gb_weights\"] = gb_weights_test\n",
    "\n",
    "\n",
    "for name, new_weights in weights.items():\n",
    "    W = numpy.concatenate([new_weights / new_weights.sum() * len(target_test), [1] * len(target_test)])\n",
    "    Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.51)\n",
    "    clf = GradientBoostingClassifier(subsample=0.3, n_estimators=50).fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "\n",
    "    print(name, roc_auc_score(Yts, clf.predict_proba(Xts)[:, 1], sample_weight=Wts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding reweighter\n",
    "\n",
    "With `FoldingReweighter` one can simpler do cross-validation and in the end obtain unbiased weights for the whole original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base reweighter\n",
    "reweighter_base = reweight.GBReweighter(\n",
    "    n_estimators=50, learning_rate=0.1, max_depth=3, min_samples_leaf=1000, gb_args={\"subsample\": 0.4}\n",
    ")\n",
    "reweighter = reweight.FoldingReweighter(reweighter_base, n_folds=2)\n",
    "# it is not needed divide data into train/test parts; rewighter can be train on the whole samples\n",
    "reweighter.fit(original, target)\n",
    "\n",
    "# predict method provides unbiased weights prediction for the whole sample\n",
    "# folding reweighter contains two reweighters, each is trained on one half of samples\n",
    "# during predictions each reweighter predicts another half of samples not used in training\n",
    "folding_weights = reweighter.predict_weights(original)\n",
    "\n",
    "draw_distributions(original, target, folding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB discrimination for reweighting rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.concatenate([original, target])\n",
    "labels = numpy.array([0] * len(original) + [1] * len(target))\n",
    "\n",
    "weights = {}\n",
    "weights[\"original\"] = original_weights\n",
    "weights[\"2-folding\"] = folding_weights\n",
    "\n",
    "\n",
    "for name, new_weights in weights.items():\n",
    "    W = numpy.concatenate([new_weights / new_weights.sum() * len(target), [1] * len(target)])\n",
    "    Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.51)\n",
    "    clf = GradientBoostingClassifier(subsample=0.3, n_estimators=30).fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "\n",
    "    print(name, roc_auc_score(Yts, clf.predict_proba(Xts)[:, 1], sample_weight=Wts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
