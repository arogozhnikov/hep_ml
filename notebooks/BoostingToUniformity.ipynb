{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting to Uniformity\n",
    "\n",
    "In physics applications frequently we need to achieve uniformity of predictions along some features.\n",
    "For instance, when testing for the existence of a new particle, we need a classifier to be uniform in background along the mass (otherwise one can get false discovery due to so-called peaking background).\n",
    "\n",
    "This notebook contains some comparison of classifiers. The target is to obtain flat effiency in __signal__ (without significally loosing quality of classification) in Dalitz features.\n",
    "\n",
    "The classifiers compared are \n",
    "\n",
    "* plain __GradientBoosting__ \n",
    "* __uBoost__\n",
    "* gradient boosting with knn-Ada loss (__UGB+knnAda__) \n",
    "* gradient boosting with FlatnessLoss (__UGB+FlatnessLoss__)\n",
    "\n",
    "We use dataset from paper about `uBoost` for demonstration purposes.\n",
    "We have a plenty of data here, so results are quite stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading data\n",
    "!wget -O ../data/dalitzdata.root -nc https://github.com/arogozhnikov/hep_ml/blob/data/data_to_download/dalitzdata.root?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are installing rep for nice plots and reports here\n",
    "!pip install rep --no-dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# this wrapper makes it possible to train on subset of features\n",
    "from rep.estimators import SklearnClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from hep_ml import gradientboosting as ugb\n",
    "from hep_ml import uboost\n",
    "from hep_ml.commonutils import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import root_numpy\n",
    "\n",
    "used_columns = [\"Y1\", \"Y2\", \"Y3\", \"M2AB\", \"M2AC\"]\n",
    "data = pandas.DataFrame(root_numpy.root2array(\"../data/dalitzdata.root\", treename=\"tree\"))\n",
    "labels = data[\"labels\"]\n",
    "data = data.drop(\"labels\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions in the Dalitz features for signal and background\n",
    "As we can see, the background is distributed mostly in the corners of Dalitz plot, <br />\n",
    "and for traditional classifiers this results in poor effieciency of signal in the corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data_frame, var_name1=\"M2AB\", var_name2=\"M2AC\", bins=40):\n",
    "    \"\"\"The function to plot 2D distribution histograms\"\"\"\n",
    "    plt.hist2d(data_frame[var_name1], data_frame[var_name2], bins=40, cmap=plt.cm.Blues)\n",
    "    plt.xlabel(var_name1)\n",
    "    plt.ylabel(var_name2)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1), plt.title(\"signal\"), plot_distribution(data[labels == 1])\n",
    "plt.subplot(1, 2, 2), plt.title(\"background\"), plot_distribution(data[labels == 0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up classifiers, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_features = [\"M2AB\", \"M2AC\"]\n",
    "train_features = [\"Y1\", \"Y2\", \"Y3\"]\n",
    "n_estimators = 150\n",
    "base_estimator = DecisionTreeClassifier(max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__uBoost__ training takes much time, so we reduce number of efficiency_steps, use prediction smoothing and run uBoost in threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rep.metaml import ClassifiersFactory\n",
    "\n",
    "classifiers = ClassifiersFactory()\n",
    "\n",
    "base_ada = GradientBoostingClassifier(max_depth=4, n_estimators=n_estimators, learning_rate=0.1)\n",
    "classifiers[\"AdaBoost\"] = SklearnClassifier(base_ada, features=train_features)\n",
    "\n",
    "\n",
    "knnloss = ugb.KnnAdaLossFunction(uniform_features, knn=10, uniform_label=1)\n",
    "ugbKnn = ugb.UGradientBoostingClassifier(\n",
    "    loss=knnloss, max_depth=4, n_estimators=n_estimators, learning_rate=0.4, train_features=train_features\n",
    ")\n",
    "classifiers[\"uGB+knnAda\"] = SklearnClassifier(ugbKnn)\n",
    "\n",
    "uboost_clf = uboost.uBoostClassifier(\n",
    "    uniform_features=uniform_features,\n",
    "    uniform_label=1,\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=n_estimators,\n",
    "    train_features=train_features,\n",
    "    efficiency_steps=12,\n",
    "    n_threads=4,\n",
    ")\n",
    "classifiers[\"uBoost\"] = SklearnClassifier(uboost_clf)\n",
    "\n",
    "flatnessloss = ugb.KnnFlatnessLossFunction(uniform_features, fl_coefficient=3.0, power=1.3, uniform_label=1)\n",
    "ugbFL = ugb.UGradientBoostingClassifier(\n",
    "    loss=flatnessloss, max_depth=4, n_estimators=n_estimators, learning_rate=0.1, train_features=train_features\n",
    ")\n",
    "classifiers[\"uGB+FL\"] = SklearnClassifier(ugbFL)\n",
    "\n",
    "\n",
    "classifiers.fit(trainX, trainY, parallel_profile=\"threads-4\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the results of training\n",
    "\n",
    "dependence of classification quality on the number of trees built (ROC AUC - an area under the ROC curve, the more the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rep.report.metrics import RocAuc\n",
    "\n",
    "report = classifiers.test_on(testX, testY)\n",
    "\n",
    "plt.ylim(0.88, 0.94)\n",
    "report.learning_curve(RocAuc(), steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDE (squared deviation of efficiency) learning curve\n",
    "SDE vs the number of built trees. SDE is a metric of nonuniformity &mdash; less is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hep_ml.metrics import BinBasedSDE, KnnBasedCvM\n",
    "\n",
    "report.learning_curve(BinBasedSDE(uniform_features, uniform_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CvM learning curve\n",
    "CvM is a metric of non-uniformity based on Cramer-von Mises distance. We are using knn (based on neighbours) version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.learning_curve(KnnBasedCvM(uniform_features, uniform_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curves after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.roc().plot(new_plot=True, figsize=[10, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal efficiency \n",
    "global cut corresponds to an average signal efficiency=0.5. In ideal case the picture shall be white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.efficiencies_2d(uniform_features, efficiency=0.5, signal_label=1, n_bins=15, labels_dict={1: \"signal\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for global efficiency = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.efficiencies_2d(uniform_features, efficiency=0.7, signal_label=1, n_bins=15, labels_dict={1: \"signal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
