{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks \n",
    "\n",
    "Neural networks inside [hep_ml](github.com/arogozhnikov/hep_ml) are very simple, but flexible. They are using [theano](http://deeplearning.net/software/theano/) library.\n",
    "\n",
    "**hep_ml.nnet** also provides tools to optimize any continuos expression as a decision function (there is an example below). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading a dataset\n",
    "downloading dataset from UCI and splitting it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cd: can't cd to toy_datasets\n",
      "File ‘../data/MiniBooNE_PID.txt’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!cd toy_datasets; wget -O ../data/MiniBooNE_PID.txt -nc MiniBooNE_PID.txt https://archive.ics.uci.edu/ml/machine-learning-databases/00199/MiniBooNE_PID.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = pandas.read_csv('../data/MiniBooNE_PID.txt', sep='\\s\\s*', skiprows=[0], header=None, engine='python')\n",
    "labels = pandas.read_csv('../data/MiniBooNE_PID.txt', sep=' ', nrows=1, header=None)\n",
    "labels = [1] * labels[1].values[0] + [0] * labels[2].values[0]\n",
    "data.columns = ['feature_{}'.format(key) for key in data.columns]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size=0.5, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training a network\n",
    "Training multilayer perceptron with one hidden layer with 5 neurons. \n",
    "In most cases, we simply use `MLPClassifier` with one or two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(epochs=500, layers=[5], loss='log_loss', random_state=None,\n",
       "       scaler='standard', trainer='irprop-', trainer_parameters=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hep_ml.nnet import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = MLPClassifier(layers=[5], epochs=500)\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test quality: 0.9713069468459772\n"
     ]
    }
   ],
   "source": [
    "proba = clf.predict_proba(test_data)\n",
    "print('Test quality:', roc_auc_score(test_labels, proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train quality: 0.9712880497811155\n"
     ]
    }
   ],
   "source": [
    "proba = clf.predict_proba(train_data)\n",
    "print('Train quality:', roc_auc_score(train_labels, proba[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your own neural network\n",
    "\n",
    "To create own neural network, one should provide activation function and define parameters of a network.\n",
    "\n",
    "You are not limited here to any kind of structure in this function, **hep_ml.nnet** will consider this as a black box for optimization.\n",
    "\n",
    "Simplest way is to override `prepare` method of `AbstractNeuralNetworkClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hep_ml.nnet import AbstractNeuralNetworkClassifier\n",
    "from theano import tensor as T\n",
    "\n",
    "class SimpleNeuralNetwork(AbstractNeuralNetworkClassifier):\n",
    "    def prepare(self):\n",
    "        # getting number of layers in input, hidden, output layers\n",
    "        # note that we support only one hidden layer here\n",
    "        n1, n2, n3 = self.layers_\n",
    "        \n",
    "        # creating parameters of neural network\n",
    "        W1 = self._create_matrix_parameter('W1', n1, n2)\n",
    "        W2 = self._create_matrix_parameter('W2', n2, n3)\n",
    "        \n",
    "        # defining activation function\n",
    "        def activation(input):\n",
    "            first = T.nnet.sigmoid(T.dot(input, W1))\n",
    "            return T.dot(first, W2)\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test quality: 0.9667676784980118\n"
     ]
    }
   ],
   "source": [
    "clf = SimpleNeuralNetwork(layers=[5], epochs=500)\n",
    "clf.fit(train_data, train_labels)\n",
    "print('Test quality:', roc_auc_score(test_labels, clf.predict_proba(test_data)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a very specific neural network\n",
    "this NN has one hidden layer, but the layer is quite strange, as it encounters correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test quality: 0.9713998457401084\n"
     ]
    }
   ],
   "source": [
    "from hep_ml.nnet import PairwiseNeuralNetwork\n",
    "clf = PairwiseNeuralNetwork(layers=[5], epochs=500)\n",
    "clf.fit(train_data, train_labels)\n",
    "print('Test quality:', roc_auc_score(test_labels, clf.predict_proba(test_data)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting very specific expressions as estimators\n",
    "One can use **hep_ml.nnet** to optimize any expressions as black-box\n",
    "for simplicity, let's assume we have only three variables: $\\text{var}_1, \\text{var}_2, \\text{var}_3.$\n",
    "\n",
    "And for some physical intuition we are sure that this is good expression to discriminate signal and background:\n",
    "$$\\text{output} = c_1 \\text{var}_1 + c_2 \\log \\left[ \\exp(\\text{var}_2 + \\text{var}_3) + \\exp(c_3) \\right] + c_4 \\dfrac{\\text{var}_3}{\\text{var}_2} + c_5 $$\n",
    "\n",
    "**Note**: I have written some random expression here, in practice it appears from physical intuition (or after looking at the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomNeuralNetwork(AbstractNeuralNetworkClassifier):\n",
    "    def prepare(self):\n",
    "        # getting number of layers in input, hidden, output layers\n",
    "        # note that we support only one hidden layer here\n",
    "        n1, n2, n3 = self.layers_        \n",
    "        # checking that we have three variables in input + constant\n",
    "        assert n1 == 3 + 1 \n",
    "        # creating parameters\n",
    "        c1 = self._create_scalar_parameter('c1')\n",
    "        c2 = self._create_scalar_parameter('c2')\n",
    "        c3 = self._create_scalar_parameter('c3')\n",
    "        c4 = self._create_scalar_parameter('c4')\n",
    "        c5 = self._create_scalar_parameter('c5')\n",
    "        \n",
    "        # defining activation function\n",
    "        def activation(input):\n",
    "            v1, v2, v3 = input[:, 0], input[:, 1], input[:, 2]\n",
    "            return c1 * v1 + c2 * T.log(T.exp(v2 + v3) + T.exp(c3)) + c4 * v3 / v2 + c5\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing custom pretransformer\n",
    "\n",
    "Below we define a very simple `scikit-learn` transformer which will transform each feature uniform to range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from rep.utils import Flattener\n",
    "\n",
    "class Uniformer(BaseEstimator, TransformerMixin):\n",
    "    # leaving only 3 features and flattening each variable\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformers = []\n",
    "        X = numpy.array(X, dtype=float)\n",
    "        for column in range(X.shape[1]):\n",
    "            self.transformers.append(Flattener(X[:, column]))\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = numpy.array(X, dtype=float)\n",
    "        assert X.shape[1] == len(self.transformers)\n",
    "        for column, trans in enumerate(self.transformers):\n",
    "            X[:, column] = trans(X[:, column])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test quality: 0.9145783471715777\n"
     ]
    }
   ],
   "source": [
    "# selecting three features to train: \n",
    "train_features = train_data.columns[:3]\n",
    "\n",
    "clf = CustomNeuralNetwork(layers=[5], epochs=1000, scaler=Uniformer())\n",
    "clf.fit(train_data[train_features], train_labels)\n",
    "\n",
    "print('Test quality:', roc_auc_score(test_labels, clf.predict_proba(test_data[train_features])[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling of neural neworks\n",
    "let's run AdaBoost algorithm over neural network. Boosting of the networks is rarely seen in practice due to the high cost and minor positive effects (but it is not senseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axelr/py36/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=MLPClassifier(epochs=100, layers=[5], loss='log_loss', random_state=None,\n",
       "       scaler=Uniformer(), trainer='irprop-', trainer_parameters=None),\n",
       "          learning_rate=1.0, n_estimators=10, random_state=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_nnet = MLPClassifier(layers=[5], scaler=Uniformer())\n",
    "clf = AdaBoostClassifier(base_estimator=base_nnet, n_estimators=10)\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test quality: 0.9771387726319765\n"
     ]
    }
   ],
   "source": [
    "print('Test quality:', roc_auc_score(test_labels, clf.predict_proba(test_data)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
