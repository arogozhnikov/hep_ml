

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Gradient boosting &mdash; hep_ml 0.7.4.dev5+gba709f3.d20250617 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/documentation_options.js?v=a931b840"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Losses for Gradient Boosting" href="losses.html" />
    <link rel="prev" title="hep_ml documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            hep_ml
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">hep_ml documentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradient boosting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.AdaLossFunction"><code class="docutils literal notranslate"><span class="pre">AdaLossFunction</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.AdaLossFunction.fit"><code class="docutils literal notranslate"><span class="pre">AdaLossFunction.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.AdaLossFunction.hessian"><code class="docutils literal notranslate"><span class="pre">AdaLossFunction.hessian()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.AdaLossFunction.negative_gradient"><code class="docutils literal notranslate"><span class="pre">AdaLossFunction.negative_gradient()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.AdaLossFunction.prepare_tree_params"><code class="docutils literal notranslate"><span class="pre">AdaLossFunction.prepare_tree_params()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.AdaLossFunction.set_fit_request"><code class="docutils literal notranslate"><span class="pre">AdaLossFunction.set_fit_request()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.BinFlatnessLossFunction"><code class="docutils literal notranslate"><span class="pre">BinFlatnessLossFunction</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.BinFlatnessLossFunction.set_fit_request"><code class="docutils literal notranslate"><span class="pre">BinFlatnessLossFunction.set_fit_request()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.KnnAdaLossFunction"><code class="docutils literal notranslate"><span class="pre">KnnAdaLossFunction</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.KnnAdaLossFunction.compute_parameters"><code class="docutils literal notranslate"><span class="pre">KnnAdaLossFunction.compute_parameters()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.KnnAdaLossFunction.set_fit_request"><code class="docutils literal notranslate"><span class="pre">KnnAdaLossFunction.set_fit_request()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.KnnFlatnessLossFunction"><code class="docutils literal notranslate"><span class="pre">KnnFlatnessLossFunction</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.KnnFlatnessLossFunction.set_fit_request"><code class="docutils literal notranslate"><span class="pre">KnnFlatnessLossFunction.set_fit_request()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.LogLossFunction"><code class="docutils literal notranslate"><span class="pre">LogLossFunction</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.LogLossFunction.fit"><code class="docutils literal notranslate"><span class="pre">LogLossFunction.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.LogLossFunction.hessian"><code class="docutils literal notranslate"><span class="pre">LogLossFunction.hessian()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.LogLossFunction.negative_gradient"><code class="docutils literal notranslate"><span class="pre">LogLossFunction.negative_gradient()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.LogLossFunction.prepare_tree_params"><code class="docutils literal notranslate"><span class="pre">LogLossFunction.prepare_tree_params()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.LogLossFunction.set_fit_request"><code class="docutils literal notranslate"><span class="pre">LogLossFunction.set_fit_request()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.RankBoostLossFunction"><code class="docutils literal notranslate"><span class="pre">RankBoostLossFunction</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.RankBoostLossFunction.fit"><code class="docutils literal notranslate"><span class="pre">RankBoostLossFunction.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.RankBoostLossFunction.hessian"><code class="docutils literal notranslate"><span class="pre">RankBoostLossFunction.hessian()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.RankBoostLossFunction.negative_gradient"><code class="docutils literal notranslate"><span class="pre">RankBoostLossFunction.negative_gradient()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.RankBoostLossFunction.prepare_new_leaves_values"><code class="docutils literal notranslate"><span class="pre">RankBoostLossFunction.prepare_new_leaves_values()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.RankBoostLossFunction.set_fit_request"><code class="docutils literal notranslate"><span class="pre">RankBoostLossFunction.set_fit_request()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.fit"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.predict"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier.predict()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.predict_proba"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier.predict_proba()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.set_fit_request"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier.set_fit_request()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.set_score_request"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier.set_score_request()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.staged_predict_proba"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingClassifier.staged_predict_proba()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingRegressor</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.fit"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingRegressor.fit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.predict"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingRegressor.predict()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.set_fit_request"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingRegressor.set_fit_request()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.set_score_request"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingRegressor.set_score_request()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.staged_predict"><code class="docutils literal notranslate"><span class="pre">UGradientBoostingRegressor.staged_predict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="losses.html">Losses for Gradient Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="uboost.html">uBoost</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metric functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="reweight.html">Reweighting algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="speedup.html">Fast predictions</a></li>
<li class="toctree-l1"><a class="reference internal" href="splot.html">sPlot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Code Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">hep_ml</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Gradient boosting</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/gb.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-hep_ml.gradientboosting">
<span id="gradient-boosting"></span><h1>Gradient boosting<a class="headerlink" href="#module-hep_ml.gradientboosting" title="Link to this heading">¶</a></h1>
<p>Gradient boosting is general-purpose algorithm proposed by Friedman <a class="reference internal" href="#gb" id="id1"><span>[GB]</span></a>.
It is one of the most efficient machine learning algorithms used for classification, regression and ranking.</p>
<p>The key idea of algorithm is iterative minimization of target <strong>loss</strong> function
by training each time one more estimator to the sequence. In this implementation decision trees are taken as such estimators.</p>
<p><strong>hep_ml</strong> provides non-standard loss functions for gradient boosting.
There are for instance, loss functions to fight with correlation or loss functions for ranking.
See  <a class="reference internal" href="losses.html#module-hep_ml.losses" title="hep_ml.losses"><code class="xref py py-class docutils literal notranslate"><span class="pre">hep_ml.losses</span></code></a> for details.</p>
<p>See also libraries: XGBoost, sklearn.ensemble.GradientBoostingClassifier</p>
<div role="list" class="citation-list">
<div class="citation" id="gb" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">GB</a><span class="fn-bracket">]</span></span>
<p>J.H. Friedman ‘Greedy function approximation: A gradient boosting machine.’, 2001.</p>
</div>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.AdaLossFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">AdaLossFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regularization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.AdaLossFunction" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">HessianLossFunction</span></code></p>
<p>AdaLossFunction is the same as Exponential Loss Function (aka exploss)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>regularization</strong> – float, penalty for leaves with few events,
corresponds roughly to the number of added events of both classes to each leaf.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.AdaLossFunction.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.AdaLossFunction.fit" title="Link to this definition">¶</a></dt>
<dd><p>This method is optional, it is called before all the others.
Heavy preprocessing should be done here.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.AdaLossFunction.hessian">
<span class="sig-name descname"><span class="pre">hessian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.hessian"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.AdaLossFunction.hessian" title="Link to this definition">¶</a></dt>
<dd><p>Returns diagonal of hessian matrix.
:param y_pred: numpy.array of shape [n_samples] with events passed in the same order as in <cite>fit</cite>.
:return: numpy.array of shape [n_sampels] with second derivatives with respect to each prediction.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.AdaLossFunction.negative_gradient">
<span class="sig-name descname"><span class="pre">negative_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.negative_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.AdaLossFunction.negative_gradient" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.AdaLossFunction.prepare_tree_params">
<span class="sig-name descname"><span class="pre">prepare_tree_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.prepare_tree_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.AdaLossFunction.prepare_tree_params" title="Link to this definition">¶</a></dt>
<dd><p>Prepares parameters for regression tree that minimizes MSE</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>y_pred</strong> – contains predictions for all the events passed to <cite>fit</cite> method,
moreover, the order should be the same</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple (tree_target, tree_weight) with target and weight to be used in decision tree</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.AdaLossFunction.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="losses.html#hep_ml.losses.AdaLossFunction" title="hep_ml.losses.AdaLossFunction"><span class="pre">AdaLossFunction</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.AdaLossFunction.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="returns">
<h2>Returns<a class="headerlink" href="#returns" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.BinFlatnessLossFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">BinFlatnessLossFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uniform_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uniform_label</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fl_coefficient</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_wrong_signs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinFlatnessLossFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.BinFlatnessLossFunction" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractFlatnessLossFunction</span></code></p>
<p>This loss function contains separately penalty for non-flatness and for bad prediction quality.
See <a class="reference internal" href="losses.html#id12" id="id2"><span>[FL]</span></a> for details.</p>
<p><span class="math notranslate nohighlight">\(\text{loss} =\text{ExpLoss} + c \times \text{FlatnessLoss}\)</span></p>
<p>FlatnessLoss computed using binning of uniform variables</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>uniform_features</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) – names of features, along which we want to obtain uniformity of predictions</p></li>
<li><p><strong>uniform_label</strong> (<em>int</em><em>|</em><em>list</em><em>[</em><em>int</em><em>]</em>) – the label(s) of classes for which uniformity is desired</p></li>
<li><p><strong>n_bins</strong> (<em>int</em>) – number of bins along each variable</p></li>
<li><p><strong>power</strong> (<em>float</em>) – the loss contains the difference <span class="math notranslate nohighlight">\(| F - F_bin |^p\)</span>, where p is power</p></li>
<li><p><strong>fl_coefficient</strong> (<em>float</em>) – multiplier for flatness_loss. Controls the tradeoff of quality vs uniformity.</p></li>
<li><p><strong>allow_wrong_signs</strong> (<em>bool</em>) – defines whether gradient may different sign from the “sign of class”
(i.e. may have negative gradient on signal). If False, values will be clipped to zero.</p></li>
</ul>
</dd>
</dl>
<div role="list" class="citation-list">
<div class="citation" id="fl" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>FL<span class="fn-bracket">]</span></span>
<p>A. Rogozhnikov et al, New approaches for boosting to uniformity
<a class="reference external" href="http://arxiv.org/abs/1410.4140">http://arxiv.org/abs/1410.4140</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.BinFlatnessLossFunction.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="losses.html#hep_ml.losses.BinFlatnessLossFunction" title="hep_ml.losses.BinFlatnessLossFunction"><span class="pre">BinFlatnessLossFunction</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.BinFlatnessLossFunction.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id3">
<h2>Parameters<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id4">
<h2>Returns<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.KnnAdaLossFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">KnnAdaLossFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uniform_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uniform_label</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">row_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnAdaLossFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.KnnAdaLossFunction" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractMatrixLossFunction</span></code></p>
<p>Modification of AdaLoss to achieve uniformity of predictions</p>
<p><span class="math notranslate nohighlight">\(\text{loss} = \sum_i w_i * exp(- \sum_j a_{ij} y_j score_j)\)</span></p>
<p><cite>A</cite> matrix is square, each row corresponds to a single event in train dataset, in each row we put ones
to the closest neighbours if this event from uniform class.
See <a class="reference internal" href="losses.html#bu" id="id5"><span>[BU]</span></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>uniform_features</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) – the features, along which uniformity is desired</p></li>
<li><p><strong>uniform_label</strong> (<em>int</em><em>|</em><em>list</em><em>[</em><em>int</em><em>]</em>) – the label (labels) of ‘uniform classes’</p></li>
<li><p><strong>knn</strong> (<em>int</em>) – the number of nonzero elements in the row, corresponding to event in ‘uniform class’</p></li>
</ul>
</dd>
</dl>
<div role="list" class="citation-list">
<div class="citation" id="bu" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">BU</a><span class="fn-bracket">]</span></span>
<p>A. Rogozhnikov et al, New approaches for boosting to uniformity
<a class="reference external" href="http://arxiv.org/abs/1410.4140">http://arxiv.org/abs/1410.4140</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.KnnAdaLossFunction.compute_parameters">
<span class="sig-name descname"><span class="pre">compute_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainX</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainY</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainW</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnAdaLossFunction.compute_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.KnnAdaLossFunction.compute_parameters" title="Link to this definition">¶</a></dt>
<dd><p>This method should be overloaded in descendant, and should return A, w (matrix and vector)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.KnnAdaLossFunction.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="losses.html#hep_ml.losses.KnnAdaLossFunction" title="hep_ml.losses.KnnAdaLossFunction"><span class="pre">KnnAdaLossFunction</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.KnnAdaLossFunction.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id6">
<h2>Parameters<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id7">
<h2>Returns<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.KnnFlatnessLossFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">KnnFlatnessLossFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uniform_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uniform_label</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbours</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fl_coefficient</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_wrong_signs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">42</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnFlatnessLossFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.KnnFlatnessLossFunction" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractFlatnessLossFunction</span></code></p>
<p>This loss function contains separately penalty for non-flatness and for bad prediction quality.
See <a class="reference internal" href="losses.html#id12" id="id8"><span>[FL]</span></a> for details.</p>
<p><span class="math notranslate nohighlight">\(\text{loss} = \text{ExpLoss} + c \times \text{FlatnessLoss}\)</span></p>
<p>FlatnessLoss computed using nearest neighbors in space of uniform features</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>uniform_features</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) – names of features, along which we want to obtain uniformity of predictions</p></li>
<li><p><strong>uniform_label</strong> (<em>int</em><em>|</em><em>list</em><em>[</em><em>int</em><em>]</em>) – the label(s) of classes for which uniformity is desired</p></li>
<li><p><strong>n_neighbours</strong> (<em>int</em>) – number of neighbors used in flatness loss</p></li>
<li><p><strong>power</strong> (<em>float</em>) – the loss contains the difference <span class="math notranslate nohighlight">\(| F - F_bin |^p\)</span>, where p is power</p></li>
<li><p><strong>fl_coefficient</strong> (<em>float</em>) – multiplier for flatness_loss. Controls the tradeoff of quality vs uniformity.</p></li>
<li><p><strong>allow_wrong_signs</strong> (<em>bool</em>) – defines whether gradient may different sign from the “sign of class”
(i.e. may have negative gradient on signal). If False, values will be clipped to zero.</p></li>
<li><p><strong>max_groups</strong> (<em>int</em>) – to limit memory consumption when training sample is large,
we randomly pick this number of points with their members.</p></li>
</ul>
</dd>
</dl>
<div role="list" class="citation-list">
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>FL<span class="fn-bracket">]</span></span>
<p>A. Rogozhnikov et al, New approaches for boosting to uniformity
<a class="reference external" href="http://arxiv.org/abs/1410.4140">http://arxiv.org/abs/1410.4140</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.KnnFlatnessLossFunction.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="losses.html#hep_ml.losses.KnnFlatnessLossFunction" title="hep_ml.losses.KnnFlatnessLossFunction"><span class="pre">KnnFlatnessLossFunction</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.KnnFlatnessLossFunction.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id10">
<h2>Parameters<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id11">
<h2>Returns<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.LogLossFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">LogLossFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regularization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.LogLossFunction" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">HessianLossFunction</span></code></p>
<p>Logistic loss function (logloss), aka binomial deviance, aka cross-entropy,
aka log-likelihood loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>regularization</strong> – float, penalty for leaves with few events,
corresponds roughly to the number of added events of both classes to each leaf.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.LogLossFunction.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.LogLossFunction.fit" title="Link to this definition">¶</a></dt>
<dd><p>This method is optional, it is called before all the others.
Heavy preprocessing should be done here.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.LogLossFunction.hessian">
<span class="sig-name descname"><span class="pre">hessian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.hessian"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.LogLossFunction.hessian" title="Link to this definition">¶</a></dt>
<dd><p>Returns diagonal of hessian matrix.
:param y_pred: numpy.array of shape [n_samples] with events passed in the same order as in <cite>fit</cite>.
:return: numpy.array of shape [n_sampels] with second derivatives with respect to each prediction.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.LogLossFunction.negative_gradient">
<span class="sig-name descname"><span class="pre">negative_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.negative_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.LogLossFunction.negative_gradient" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.LogLossFunction.prepare_tree_params">
<span class="sig-name descname"><span class="pre">prepare_tree_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.prepare_tree_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.LogLossFunction.prepare_tree_params" title="Link to this definition">¶</a></dt>
<dd><p>Prepares parameters for regression tree that minimizes MSE</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>y_pred</strong> – contains predictions for all the events passed to <cite>fit</cite> method,
moreover, the order should be the same</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple (tree_target, tree_weight) with target and weight to be used in decision tree</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.LogLossFunction.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="losses.html#hep_ml.losses.LogLossFunction" title="hep_ml.losses.LogLossFunction"><span class="pre">LogLossFunction</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.LogLossFunction.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id12">
<h2>Parameters<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id13">
<h2>Returns<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.RankBoostLossFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">RankBoostLossFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request_column</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty_power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.RankBoostLossFunction" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">HessianLossFunction</span></code></p>
<p>RankBoostLossFunction is target of optimization in RankBoost <a class="reference internal" href="losses.html#rb" id="id14"><span>[RB]</span></a> algorithm,
which was developed for ranking and introduces penalties for wrong order of predictions.</p>
<p>However, this implementation goes further and there is selection of optimal leaf values based
on iterative procedure. This implementation also uses matrix decomposition of loss function,
which is very effective, when labels are from some very limited set (usually it is 0, 1, 2, 3, 4)</p>
<p><span class="math notranslate nohighlight">\(\text{loss} = \sum_{ij} w_{ij} exp(pred_i - pred_j)\)</span>,</p>
<p><span class="math notranslate nohighlight">\(w_{ij} = ( \alpha + \beta * [query_i = query_j]) R_{label_i, label_j}\)</span>, where
<span class="math notranslate nohighlight">\(R_{ij} = 0\)</span> if <span class="math notranslate nohighlight">\(i \leq j\)</span>, else <span class="math notranslate nohighlight">\(R_{ij} = (i - j)^{p}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request_column</strong> (<em>str</em>) – name of column with search query ids. The higher attention is payed
to samples with same query.</p></li>
<li><p><strong>penalty_power</strong> (<em>float</em>) – describes dependence of penalty on the difference between target labels.</p></li>
<li><p><strong>update_iterations</strong> (<em>int</em>) – number of minimization steps to provide optimal values in leaves.</p></li>
</ul>
</dd>
</dl>
<div role="list" class="citation-list">
<div class="citation" id="rb" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">RB</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="25">
<li><p>Freund et al. An Efficient Boosting Algorithm for Combining Preferences</p></li>
</ol>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.RankBoostLossFunction.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.RankBoostLossFunction.fit" title="Link to this definition">¶</a></dt>
<dd><p>This method is optional, it is called before all the others.
Heavy preprocessing should be done here.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.RankBoostLossFunction.hessian">
<span class="sig-name descname"><span class="pre">hessian</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.hessian"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.RankBoostLossFunction.hessian" title="Link to this definition">¶</a></dt>
<dd><p>Returns diagonal of hessian matrix.
:param y_pred: numpy.array of shape [n_samples] with events passed in the same order as in <cite>fit</cite>.
:return: numpy.array of shape [n_sampels] with second derivatives with respect to each prediction.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.RankBoostLossFunction.negative_gradient">
<span class="sig-name descname"><span class="pre">negative_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.negative_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.RankBoostLossFunction.negative_gradient" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.RankBoostLossFunction.prepare_new_leaves_values">
<span class="sig-name descname"><span class="pre">prepare_new_leaves_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">terminal_regions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.prepare_new_leaves_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.RankBoostLossFunction.prepare_new_leaves_values" title="Link to this definition">¶</a></dt>
<dd><p>This expression comes from optimization of second-order approximation of loss function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.RankBoostLossFunction.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="losses.html#hep_ml.losses.RankBoostLossFunction" title="hep_ml.losses.RankBoostLossFunction"><span class="pre">RankBoostLossFunction</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.RankBoostLossFunction.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id15">
<h2>Parameters<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id16">
<h2>Returns<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">UGradientBoostingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_tree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingClassifier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">UGradientBoostingBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ClassifierMixin</span></code></p>
<p>This version of gradient boosting supports only two-class classification and only special losses
derived from AbstractLossFunction.</p>
<p><cite>max_depth</cite>, <cite>max_leaf_nodes</cite>, <cite>min_samples_leaf</cite>, <cite>min_samples_split</cite>, <cite>max_features</cite> are parameters
of regression tree, which is used as base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference internal" href="losses.html#hep_ml.losses.AbstractLossFunction" title="hep_ml.losses.AbstractLossFunction"><em>AbstractLossFunction</em></a>) – any descendant of AbstractLossFunction, those are very various.
See <a class="reference internal" href="losses.html#module-hep_ml.losses" title="hep_ml.losses"><code class="xref py py-class docutils literal notranslate"><span class="pre">hep_ml.losses</span></code></a> for available losses.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em>) – number of trained trees.</p></li>
<li><p><strong>subsample</strong> (<em>float</em>) – fraction of data to use on each stage</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – size of step.</p></li>
<li><p><strong>update_tree</strong> (<em>bool</em>) – True by default. If False, ‘improvement’ step after fitting tree will be skipped.</p></li>
<li><p><strong>train_features</strong> – features used by tree.
Note that algorithm may require also variables used by loss function, but not listed here.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingClassifier.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.fit" title="Link to this definition">¶</a></dt>
<dd><p>Train formula.
Only two-class binary classification is supported with labels 0 and 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – dataset of shape [n_samples, n_features]</p></li>
<li><p><strong>y</strong> – labels, array-like of shape [n_samples]</p></li>
<li><p><strong>sample_weight</strong> – array-like of shape [n_samples] or None</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingClassifier.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predicted classes for each event</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – pandas.DataFrame with all train_features</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>numpy.array of shape [n_samples] with predicted classes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingClassifier.predict_proba"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predicted probabilities for each event</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – pandas.DataFrame with all train_features</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>numpy.array of shape [n_samples, n_classes]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier" title="hep_ml.gradientboosting.UGradientBoostingClassifier"><span class="pre">UGradientBoostingClassifier</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id17">
<h2>Parameters<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id18">
<h2>Returns<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier.set_score_request">
<span class="sig-name descname"><span class="pre">set_score_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingClassifier" title="hep_ml.gradientboosting.UGradientBoostingClassifier"><span class="pre">UGradientBoostingClassifier</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.set_score_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">score</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">score</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">score</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id19">
<h2>Parameters<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">score</span></code>.</p>
</dd>
</dl>
</section>
<section id="id20">
<h2>Returns<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingClassifier.staged_predict_proba">
<span class="sig-name descname"><span class="pre">staged_predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingClassifier.staged_predict_proba"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingClassifier.staged_predict_proba" title="Link to this definition">¶</a></dt>
<dd><p>Predicted probabilities for each event</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – data</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>sequence of numpy.array of shape [n_samples, n_classes]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">hep_ml.gradientboosting.</span></span><span class="sig-name descname"><span class="pre">UGradientBoostingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_tree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingRegressor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingRegressor" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">UGradientBoostingBase</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">RegressorMixin</span></code></p>
<p>Gradient Boosted regressor. Approximates target by sum of predictions of several trees.</p>
<p><cite>max_depth</cite>, <cite>max_leaf_nodes</cite>, <cite>min_samples_leaf</cite>, <cite>min_samples_split</cite>, <cite>max_features</cite> are parameters
of regression tree, which is used as base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference internal" href="losses.html#hep_ml.losses.AbstractLossFunction" title="hep_ml.losses.AbstractLossFunction"><em>AbstractLossFunction</em></a>) – any descendant of AbstractLossFunction, those are very various.
See <a class="reference internal" href="losses.html#module-hep_ml.losses" title="hep_ml.losses"><code class="xref py py-class docutils literal notranslate"><span class="pre">hep_ml.losses</span></code></a> for available losses.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em>) – number of trained trees.</p></li>
<li><p><strong>subsample</strong> (<em>float</em>) – fraction of data to use on each stage</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em>) – size of step.</p></li>
<li><p><strong>update_tree</strong> (<em>bool</em>) – True by default. If False, ‘improvement’ step after fitting tree will be skipped.</p></li>
<li><p><strong>train_features</strong> – features used by tree.
Note that algorithm may require also variables used by loss function, but not listed here.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingRegressor.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.fit" title="Link to this definition">¶</a></dt>
<dd><p>Fit estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – dataset of shape [n_samples, n_features]</p></li>
<li><p><strong>y</strong> – target values, array-like of shape [n_samples]</p></li>
<li><p><strong>sample_weight</strong> – array-like of shape [n_samples] or None</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingRegressor.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.predict" title="Link to this definition">¶</a></dt>
<dd><p>Predict values for new samples</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – pandas.DataFrame with all train_features</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>numpy.array of shape [n_samples]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingRegressor.set_fit_request">
<span class="sig-name descname"><span class="pre">set_fit_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor" title="hep_ml.gradientboosting.UGradientBoostingRegressor"><span class="pre">UGradientBoostingRegressor</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.set_fit_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id21">
<h2>Parameters<a class="headerlink" href="#id21" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
</dd>
</dl>
</section>
<section id="id22">
<h2>Returns<a class="headerlink" href="#id22" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingRegressor.set_score_request">
<span class="sig-name descname"><span class="pre">set_score_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'$UNCHANGED$'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#hep_ml.gradientboosting.UGradientBoostingRegressor" title="hep_ml.gradientboosting.UGradientBoostingRegressor"><span class="pre">UGradientBoostingRegressor</span></a></span></span><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.set_score_request" title="Link to this definition">¶</a></dt>
<dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class="pre">score</span></code> method.</p>
<p>Note that this method is only relevant if
<code class="docutils literal notranslate"><span class="pre">enable_metadata_routing=True</span></code> (see <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.set_config()</span></code>).
Please see <span class="xref std std-ref">User Guide</span> on how the routing
mechanism works.</p>
<p>The options for each parameter are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class="pre">score</span></code> if provided. The request is ignored if metadata is not provided.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class="pre">score</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li>
</ul>
<p>The default (<code class="docutils literal notranslate"><span class="pre">sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the
existing request. This allows you to change the request for some
parameters and not others.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 1.3.</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only relevant if this estimator is used as a
sub-estimator of a meta-estimator, e.g. used inside a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>. Otherwise it has no effect.</p>
</div>
<section id="id23">
<h2>Parameters<a class="headerlink" href="#id23" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>sample_weight<span class="classifier">str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">score</span></code>.</p>
</dd>
</dl>
</section>
<section id="id24">
<h2>Returns<a class="headerlink" href="#id24" title="Link to this heading">¶</a></h2>
<dl class="simple">
<dt>self<span class="classifier">object</span></dt><dd><p>The updated object.</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="hep_ml.gradientboosting.UGradientBoostingRegressor.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#UGradientBoostingRegressor.staged_predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hep_ml.gradientboosting.UGradientBoostingRegressor.staged_predict" title="Link to this definition">¶</a></dt>
<dd><p>Return predictions after each new tree</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – data</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>sequence of numpy.array of shape [n_samples]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="hep_ml documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="losses.html" class="btn btn-neutral float-right" title="Losses for Gradient Boosting" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2015-2017, Yandex; Alex Rogozhnikov and contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
